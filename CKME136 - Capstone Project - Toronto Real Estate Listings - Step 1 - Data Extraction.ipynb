{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKME136 - Capstone Project - Toronto Real Estate Listings\n",
    "## Step 1: Data Extraction - Web Scraping with requests & BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1A. Imports for webscraping and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# Beautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1B. Create a list of all toronto listing pages\n",
    "    \n",
    "- example list page https://listing.ca/mls/?.cy.........12..$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_page_urls = []\n",
    "def all_listing_page_urls(url_start, url_end, pages):\n",
    "    for i in range(1,pages):\n",
    "        generated_urls = url_start+str(i)+url_end\n",
    "        listing_page_urls.append(generated_urls)\n",
    "\n",
    "\n",
    "toronto_listing_url_start = 'https://listing.ca/mls/?.cy.........'\n",
    "toronto_listing_url_end = '..$'\n",
    "max_pages = 175\n",
    "\n",
    "all_listing_page_urls(toronto_listing_url_start, toronto_listing_url_end, max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1C. Create CSV of specific listing urls\n",
    "    \n",
    "- https://toronto.listing.ca/168-bonis-ave-1211.E4349723.htm#15-1dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_listing_urls(listing_page_urls):\n",
    "    all_page_text = []\n",
    "    \n",
    "    delays = [0.3, 0.4, 0.7, 0.9, 1.0, 1.3]\n",
    "\n",
    "    #extract html from list of page urls\n",
    "    for page_url in listing_page_urls:\n",
    "        resp = requests.get(page_url).text\n",
    "        soup = BeautifulSoup(resp)\n",
    "        #find 'a' tags\n",
    "        page_text = soup.find_all('a')\n",
    "        all_page_text.extend(page_text)\n",
    "        \n",
    "        #sleep between requests to avoid getting blocked\n",
    "        delay = np.random.choice(delays)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "    #extract urls from each text\n",
    "    urls = [x.get(\"href\") for x in all_page_text]\n",
    "    #regex for urls that have street numbers\n",
    "    r = re.compile('https://toronto.listing.ca/[0-9]')\n",
    "    #create unique list of listing urls\n",
    "    individual_listing_urls = list(set(filter(r.match, urls)))\n",
    "    \n",
    "    #save urls in a csv\n",
    "    with open('individual_listing_urls.csv','w') as f1:\n",
    "        writer = csv.writer(f1, delimiter=',', lineterminator='\\n',)\n",
    "        writer.writerow(individual_listing_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv of all listing urls\n",
    "individual_listing_urls(listing_page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1D. Create individual text files of raw html text for each listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_indiviudal_listing_scraper():\n",
    "    \n",
    "    with open(\"individual_listing_urls.csv\",'r') as f1:\n",
    "        reader = csv.reader(f1)\n",
    "        individual_listing_urls = list(itertools.chain.from_iterable(zip(*reader)))\n",
    "        delays = [0.3, 0.4, 0.7, 0.9, 1.0, 1.3, 1.5]\n",
    "        \n",
    "        for i, listing in enumerate(individual_listing_urls):\n",
    "            txt_file= open(\"listing_html_\"+str(i)+\".txt\",\"w\")\n",
    "            resp = requests.get(listing).text.encode('utf-8')\n",
    "            txt_file.write(resp)\n",
    "            txt_file.close()\n",
    "        \n",
    "            #sleep between requests to avoid getting blocked\n",
    "            delay = np.random.choice(delays)\n",
    "            if i % 7 == 0:\n",
    "                time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_indiviudal_listing_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "5. Create sample listing_url CSV and to create unique key set\n",
    "\n",
    "- Use sample of 100 listings to determine key set for rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sample csv\n",
    "with open('individual_listing_urls.csv','r') as f1:\n",
    "    reader = csv.reader(f1)\n",
    "    individual_listing_urls = []\n",
    "    for row in reader:\n",
    "        individual_listing_urls.extend(row)\n",
    "    \n",
    "    sample_listing_urls = individual_listing_urls[:100]\n",
    "    \n",
    "    with open('sample_individual_listing_urls.csv','w') as f2:\n",
    "        writer = csv.writer(f2, delimiter=',', lineterminator='\\n',)\n",
    "        writer.writerow(sample_listing_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'OFFICE', u'FAMILY ROOM', u'DINING ROOM', u'DEN', u'REC', u'FOYER', u'2ND BEDROOM', u'BATHROOM', u'BATHROOMS', u'LIBRARY', u'EXERCISE', u'KITCHEN', u'LIVING ROOM', u'MASTER BEDROOM', u'STUDY', u'4TH BEDROOM', u'3RD BEDROOM', u'OTHER', u'LEGAL DESCRIPTION', u'5TH BEDROOM', u'SUNROOM', u'GREAT ROOM', u'LAUNDRY', u'SITTING', u'SOLARIUM', u'LOCKER', u'BEDROOM', u'BREAKFAST', u'MEDIA/ENT', u'UTILITY']\n"
     ]
    }
   ],
   "source": [
    "# print list of unique keys\n",
    "def key_finder_indiviudal_listing_scraper():\n",
    "    with open(\"sample_individual_listing_urls.csv\",'r') as f1:\n",
    "        reader = csv.reader(f1)\n",
    "        individual_listing_urls = list(itertools.chain.from_iterable(zip(*reader)))\n",
    "        \n",
    "    all_rooms_dicts = []\n",
    "\n",
    "    for i, listing_url in enumerate(individual_listing_urls):\n",
    "        with open(\"listing_html_\"+str(i)+\".txt\",'r') as listing:\n",
    "            soup = BeautifulSoup(listing, 'html.parser')\n",
    "            \n",
    "            regex = re.compile('([0-9]){2}')\n",
    "            content = soup.findAll(\"div\", {\"class\": \"lpc15\"})[2].text\n",
    "            if re.match(regex, content) is None:\n",
    "                start_range = 8\n",
    "            else:\n",
    "                start_range = 10\n",
    "                \n",
    "            rooms = soup.findAll(\"div\", {\"class\": \"lpc15\"})[start_range:]\n",
    "            rooms_string = \"\".join(map(str, rooms))\n",
    "            rooms_soup = BeautifulSoup(rooms_string, 'html.parser')\n",
    "            rooms_soup_range = len(rooms_soup)\n",
    "            \n",
    "            room_keys = []\n",
    "            \n",
    "            for i in range(start_range, start_range+rooms_soup_range):\n",
    "                value = soup.findAll(\"div\", {\"class\": \"lpc15\"})[i].text\n",
    "                key_extract = str(soup.findAll(\"div\", {\"class\": \"lpc15\"})[i].previousSibling)\n",
    "                key = BeautifulSoup(key_extract).text\n",
    "                \n",
    "                room_keys.append(key)\n",
    "            \n",
    "            all_rooms_dicts.extend(room_keys)\n",
    "            \n",
    "    print list(set(all_rooms_dicts))\n",
    "\n",
    "key_finder_indiviudal_listing_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1E. Extract rooms features from individual listing urls\n",
    "    \n",
    "- ex. FAMILY ROOM, BEDROOM, BATHROOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to extract rooms data from a listing\n",
    "def rooms_indiviudal_listing_scraper():\n",
    "    with open(\"individual_listing_urls.csv\",'r') as f1:\n",
    "        reader = csv.reader(f1)\n",
    "        individual_listing_urls = list(itertools.chain.from_iterable(zip(*reader)))\n",
    "        \n",
    "    all_rooms_dicts = []\n",
    "\n",
    "    for i, listing_url in enumerate(individual_listing_urls):\n",
    "        with open(\"listing_html_\"+str(i)+\".txt\",'r') as listing:\n",
    "            soup = BeautifulSoup(listing, 'html.parser')\n",
    "            \n",
    "            regex = re.compile('([0-9]){2}')\n",
    "            content = soup.findAll(\"div\", {\"class\": \"lpc15\"})[2].text\n",
    "            if re.match(regex, content) is None:\n",
    "                start_range = 8\n",
    "            else:\n",
    "                start_range = 10\n",
    "                \n",
    "            rooms = soup.findAll(\"div\", {\"class\": \"lpc15\"})[start_range:]\n",
    "            rooms_string = \"\".join(map(str, rooms))\n",
    "            rooms_soup = BeautifulSoup(rooms_string, 'html.parser')\n",
    "            rooms_soup_range = len(rooms_soup)\n",
    "            \n",
    "            room_keys = ['listing_url']\n",
    "            room_values = [listing_url]\n",
    "            \n",
    "            for i in range(start_range, start_range+rooms_soup_range):\n",
    "                value = soup.findAll(\"div\", {\"class\": \"lpc15\"})[i].text\n",
    "                key_extract = str(soup.findAll(\"div\", {\"class\": \"lpc15\"})[i].previousSibling)\n",
    "                key = BeautifulSoup(key_extract).text\n",
    "                \n",
    "                \n",
    "                room_keys.append(key)\n",
    "                room_values.append(value)\n",
    "                \n",
    "            room_dict = dict(zip(room_keys, room_values))\n",
    "            \n",
    "            all_rooms_dicts.append(room_dict)\n",
    "            \n",
    "            listing.close()\n",
    "            \n",
    "    with open('individual_listing_rooms.csv','wb') as f2:\n",
    "        keys = ['listing_url', 'OFFICE', 'FAMILY ROOM', 'DINING ROOM', 'DEN', 'REC', 'FOYER', '2ND BEDROOM', 'BATHROOM', 'BATHROOMS', 'LIBRARY', 'EXERCISE', 'KITCHEN', 'LIVING ROOM', 'MASTER BEDROOM', 'STUDY', '4TH BEDROOM', '3RD BEDROOM', 'OTHER', 'LEGAL DESCRIPTION', '5TH BEDROOM', 'SUNROOM', 'GREAT ROOM', 'LAUNDRY', 'SITTING', 'SOLARIUM', 'LOCKER', 'BEDROOM', 'BREAKFAST', 'MEDIA/ENT', 'UTILITY']\n",
    "        dict_writer = csv.DictWriter(f2, keys, extrasaction = 'ignore')\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(all_rooms_dicts)\n",
    "\n",
    "rooms_indiviudal_listing_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1F. Extract other listing features from individual listing urls\n",
    "\n",
    "- ex. comparable sold data, listing price, dwelling type, address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract sold comparable data\n",
    "def sold_data_1(data):\n",
    "    try:\n",
    "        return comparables_class.find(text=re.compile(data)).findNext('div').contents[0]\n",
    "    except:\n",
    "        return '0'\n",
    "\n",
    "def sold_data_2(data):\n",
    "    try:\n",
    "        return comparables_class.find(text=re.compile(data)).findNext(text=re.compile(data)).findNext('div').contents[0]\n",
    "    except:\n",
    "        return '0'\n",
    "\n",
    "def sold_data_3(data):\n",
    "    try:\n",
    "        return comparables_class.find(text=re.compile(data)).findNext(text=re.compile(data)).findNext(text=re.compile(data)).findNext('div').contents[0]\n",
    "    except:\n",
    "        return '0'\n",
    "\n",
    "def sold_data_4(data):\n",
    "    try:\n",
    "        return comparables_class.find(text=re.compile(data)).findNext(text=re.compile(data)).findNext(text=re.compile(data)).findNext(text=re.compile(data)).findNext('div').contents[0]\n",
    "    except:\n",
    "        return '0'\n",
    "\n",
    "def sold_data_5(data):\n",
    "    try:\n",
    "        return comparables_class.find(text=re.compile(data)).findNext(text=re.compile(data)).findNext(text=re.compile(data)).findNext(text=re.compile(data)).findNext(text=re.compile(data)).findNext('div').contents[0]\n",
    "    except:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-287-102fe599e9eb>:28: SyntaxWarning: name 'comparables_class' is assigned to before global declaration\n",
      "  global comparables_class\n"
     ]
    }
   ],
   "source": [
    "#to extract features from a listing including sale price, sold comparables, listing url, address, community, type of dwelling, property, features, description, extras\n",
    "def other_features_indiviudal_listing_scraper():\n",
    "    with open(\"individual_listing_urls.csv\",'r') as f1:\n",
    "        reader = csv.reader(f1)\n",
    "        individual_listing_urls = list(itertools.chain.from_iterable(zip(*reader)))\n",
    "        \n",
    "        all_other_features = []\n",
    "\n",
    "        for i, listing_url in enumerate(individual_listing_urls):\n",
    "            with open(\"listing_html_\"+str(i)+\".txt\",'r') as listing:\n",
    "                soup = BeautifulSoup(listing, 'html.parser')\n",
    "                \n",
    "                #listing url and MLS ID\n",
    "                x = [listing_url]\n",
    "                \n",
    "                #sale price and listing date\n",
    "                x.append(soup.find(\"div\", {\"class\": \"sales\"}).findChildren()[3].text)\n",
    "                x.append(soup.find(\"div\", {\"class\": \"sales\"}).findChildren()[1].text)\n",
    "                \n",
    "                #address, community\n",
    "                x.extend([soup.findAll(\"div\", {\"class\": \"lpc15\"})[0].text])\n",
    "                \n",
    "                #type of dwelling\n",
    "                x.extend([soup.findAll(\"div\", {\"class\": \"lpc15\"})[1].text])\n",
    "                \n",
    "                #Comparables\n",
    "                comparables_class = soup.find(\"div\", {\"class\": \"sales\", \"class\": \"comparables\"})\n",
    "                global comparables_class\n",
    "                \n",
    "                comparable_sold_price_1 = sold_data_1('Sold Price')\n",
    "                comparable_list_price_1 = sold_data_1('List Price')\n",
    "                comparable_sold_date_1 = sold_data_1('Sold Date')\n",
    "                \n",
    "                comparable_sold_price_2 = sold_data_2('Sold Price')\n",
    "                comparable_list_price_2 = sold_data_2('List Price')\n",
    "                comparable_sold_date_2 = sold_data_2('Sold Date')\n",
    "                \n",
    "                comparable_sold_price_3 = sold_data_3('Sold Price')\n",
    "                comparable_list_price_3 = sold_data_3('List Price')\n",
    "                comparable_sold_date_3 = sold_data_3('Sold Date')\n",
    "                \n",
    "                comparable_sold_price_4 = sold_data_4('Sold Price')\n",
    "                comparable_list_price_4 = sold_data_4('List Price')\n",
    "                comparable_sold_date_4 = sold_data_4('Sold Date')\n",
    "                \n",
    "                comparable_sold_price_5 = sold_data_5('Sold Price')\n",
    "                comparable_list_price_5 = sold_data_5('List Price')\n",
    "                comparable_sold_date_5 = sold_data_5('Sold Date')\n",
    "\n",
    "                x.extend([comparable_sold_price_1,\n",
    "                          comparable_list_price_1,\n",
    "                          comparable_sold_date_1,\n",
    "                          comparable_sold_price_2,\n",
    "                          comparable_list_price_2,\n",
    "                          comparable_sold_date_2,\n",
    "                          comparable_sold_price_3,\n",
    "                          comparable_list_price_3,\n",
    "                          comparable_sold_date_3,\n",
    "                          comparable_sold_price_4,\n",
    "                          comparable_list_price_4,\n",
    "                          comparable_sold_date_4,\n",
    "                          comparable_sold_price_5,\n",
    "                          comparable_list_price_5,\n",
    "                          comparable_sold_date_5])\n",
    "\n",
    "                \n",
    "                #property features, description, and extras\n",
    "                #required to account for some listings with property layout element\n",
    "                regex = re.compile('([0-9]){2}')\n",
    "                content = soup.findAll(\"div\", {\"class\": \"lpc15\"})[2].text\n",
    "                if re.match(regex, content) is None:\n",
    "                    try:\n",
    "                        x.append([soup.findAll(\"div\", {\"class\": \"lpc15\"})[2].text])\n",
    "                    except:\n",
    "                        x.append(None)\n",
    "                    try:\n",
    "                        x.append([soup.findAll(\"div\", {\"class\": \"lpc15\"})[6].text])\n",
    "                    except:\n",
    "                        x.append(None)\n",
    "                    try:\n",
    "                        x.append([soup.findAll(\"div\", {\"class\": \"lpc15\"})[7].text])\n",
    "                    except:\n",
    "                        x.append(None)\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        x.append([soup.findAll(\"div\", {\"class\": \"lpc15\"})[3].text])\n",
    "                    except:\n",
    "                        x.append(None)\n",
    "                    try:\n",
    "                        x.append([soup.findAll(\"div\", {\"class\": \"lpc15\"})[7].text])\n",
    "                    except:\n",
    "                        x.append(None)\n",
    "                    try:\n",
    "                        x.append([soup.findAll(\"div\", {\"class\": \"lpc15\"})[8].text])\n",
    "                    except:\n",
    "                        x.append(None)\n",
    "                \n",
    "                listing.close()\n",
    "                \n",
    "                all_other_features.append(x)\n",
    "                \n",
    "        with open('individual_listing_other_features_v3.csv','w') as f2:\n",
    "            writer = csv.writer(f2, delimiter=',',lineterminator='\\n',)\n",
    "            \n",
    "            keys = ['listing_url', 'listing_price', 'listing_date', 'address', 'dwelling_type', \n",
    "                    'comparable_sold_price_1', 'comparable_list_price_1','comparable_sold_date_1','comparable_sold_price_2','comparable_list_price_2','comparable_sold_date_2','comparable_sold_price_3','comparable_list_price_3','comparable_sold_date_3','comparable_sold_price_4','comparable_list_price_4','comparable_sold_date_4','comparable_sold_price_5','comparable_list_price_5','comparable_sold_date_5',\n",
    "                   'listing_features', 'listing_description', 'listing_extras']\n",
    "            writer.writerow(keys)\n",
    "            writer.writerows(all_other_features)\n",
    "\n",
    "other_features_indiviudal_listing_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
